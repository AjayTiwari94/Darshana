'use client'

import React, { useState, useRef, useEffect, useCallback, useMemo } from 'react'
import { motion, AnimatePresence } from 'framer-motion'
import { 
  Send, 
  Mic, 
  MicOff, 
  Volume2, 
  VolumeX, 
  Bot,
  User,
  Loader,
  Sparkles,
  MapPin,
  BookOpen,
  Camera,
  AlertCircle,
  RotateCcw,
  Headphones,
  StopCircle,
  X
} from 'lucide-react'
import { useNaradAIStore, useUIStore } from '@/store'
import { useRouter } from 'next/navigation'
import { v4 as uuidv4 } from 'uuid'
import { ComprehensiveTTSService } from '@/services/tts/comprehensiveTTS'

// Enhanced markdown processor for headings and other formatting
export const processMarkdown = (content: string) => {
  // Handle null or undefined content
  if (!content) {
    console.log('processMarkdown received null/undefined content');
    return null;
  }
  
  console.log('processMarkdown processing content:', content);
  
  // Split content by newlines
  const lines = content.split('\n')
  const processedLines = []
  
  for (let i = 0; i < lines.length; i++) {
    const line = lines[i]
    
    // Check for ## headings
    if (line.startsWith('## ')) {
      const headingText = line.substring(3)
      processedLines.push(
        <h3 key={i} className="markdown-heading text-lg">
          {headingText}
        </h3>
      )
    } 
    // Check for ### subheadings
    else if (line.startsWith('### ')) {
      const headingText = line.substring(4)
      processedLines.push(
        <h4 key={i} className="markdown-subheading text-md">
          {headingText}
        </h4>
      )
    }
    // Handle complex formatting (bold, italic, etc.)
    else if (line.includes('**') || line.includes('*')) {
      // Process the line for formatting
      const processedLine = processInlineFormatting(line)
      processedLines.push(
        <p key={i} className="markdown-paragraph">
          {processedLine}
        </p>
      )
    }
    // Handle empty lines
    else if (line.trim() === '') {
      processedLines.push(<br key={i} />)
    }
    // Handle regular paragraphs
    else {
      processedLines.push(
        <p key={i} className="markdown-paragraph">
          {line}
        </p>
      )
    }
  }
  
  console.log('processMarkdown result:', processedLines);
  return processedLines
}

// Process inline formatting (bold, italic, etc.)
const processInlineFormatting = (text: string) => {
  if (!text) return text;
  
  // We'll process formatting in a specific order to handle nesting correctly
  // First process bold (**), then italic (*)
  let result: (string | JSX.Element)[] = [text]
  
  // Process bold formatting (**)
  result = processFormatting(result, '**', 'bold')
  
  // Process italic formatting (*)
  result = processFormatting(result, '*', 'italic')
  
  // If we only have one element and it's a string, return it directly
  if (result.length === 1 && typeof result[0] === 'string') {
    return result[0]
  }
  
  // Otherwise, return the array of elements
  return result;
}

// Generic function to process formatting
const processFormatting = (elements: (string | JSX.Element)[], marker: string, type: string) => {
  const result: (string | JSX.Element)[] = []
  
  elements.forEach((element, index) => {
    // Only process string elements
    if (typeof element === 'string') {
      const parts = element.split(marker)
      
      // If we don't have enough parts for formatting, just add the element as is
      if (parts.length < 3) {
        result.push(element)
        return
      }
      
      // Process the parts
      for (let i = 0; i < parts.length; i++) {
        // Even indices (0, 2, 4, ...) are regular text
        if (i % 2 === 0) {
          if (parts[i] !== '') {
            result.push(parts[i])
          }
        } 
        // Odd indices (1, 3, 5, ...) are formatted text
        else {
          if (type === 'bold') {
            result.push(<strong key={`${index}-${i}`} className="markdown-bold">{parts[i]}</strong>)
          } else if (type === 'italic') {
            result.push(<em key={`${index}-${i}`} className="markdown-italic">{parts[i]}</em>)
          }
        }
      }
    } else {
      // Non-string elements (like already processed elements) are added as is
      result.push(element)
    }
  })
  
  return result;
}

const NaradAIChatComponent = () => {
  const router = useRouter()
  const { 
    session, 
    messages, 
    isActive, 
    isLoading, 
    sendMessage, 
    startSession, 
    endSession,
    initialInput,
    setInitialInput,
    updateSessionContext,
    clearMessages,
    // ✅ Add speech state from store
    isSpeaking,
    readingMessageId,
    setIsSpeaking,
    setReadingMessageId
  } = useNaradAIStore()
  
  const { naradAIOpen, setNaradAIOpen } = useUIStore()
  
  const [inputMessage, setInputMessage] = useState('')
  const [isMinimized, setIsMinimized] = useState(false)
  const [isRecording, setIsRecording] = useState(false)
  const [audioEnabled, setAudioEnabled] = useState(true)
  const [voiceToVoiceMode, setVoiceToVoiceMode] = useState(false) // Keep but disable functionality
  const [showSuggestions, setShowSuggestions] = useState(true)
  const [error, setError] = useState<string | null>(null)
  const [speechSupported, setSpeechSupported] = useState(false)
  const [interimTranscript, setInterimTranscript] = useState('')
  const [selectedSpeechLang, setSelectedSpeechLang] = useState<'hi' | 'en' | 'bn' | 'ta' | 'te'>('en')
  // TTS selection state
  const [selectedTTS, setSelectedTTS] = useState<'vapi' | 'vapi-web' | 'elevenlabs' | 'web-speech'>('vapi')
  const recognitionRef = useRef<any>(null)
  const synthRef = useRef<any>(null)
  const audioRef = useRef<HTMLAudioElement | null>(null)
  const currentUtteranceRef = useRef<any>(null)
  const isProcessingRef = useRef(false)
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null)
  const isSpeakingRequestedRef = useRef(false)
  const speechQueueRef = useRef<string[]>([])
  const isSpeakingQueueRef = useRef(false)
  const isTogglingVoiceModeRef = useRef(false)

  const messagesEndRef = useRef<HTMLDivElement>(null)
  const inputRef = useRef<HTMLInputElement>(null)
  
  // Initialize the TTS service
  const ttsService = useMemo(() => {
    const service = new ComprehensiveTTSService(selectedTTS);
    service.setAudioRef(audioRef);
    service.setOnStopCallback(() => {
      setIsSpeaking(false);
      setReadingMessageId(null);
    });
    return service;
  }, [selectedTTS, setIsSpeaking, setReadingMessageId]);
  
  // Function to detect language from text
  const detectLanguageFromText = (text: string): 'hi' | 'en' | 'bn' | 'ta' | 'te' => {
    // Hindi characters range
    if (/[\u0900-\u097F]/.test(text)) {
      return 'hi';
    }
    // Bengali characters range
    else if (/[\u0980-\u09FF]/.test(text)) {
      return 'bn';
    }
    // Tamil characters range
    else if (/[\u0B80-\u0BFF]/.test(text)) {
      return 'ta';
    }
    // Telugu characters range
    else if (/[\u0C00-\u0C7F]/.test(text)) {
      return 'te';
    }
    // Default to English
    else {
      return 'en';
    }
  };

  // Function to get speech language
  const getSpeechLang = () => {
    // Map selector short codes to BCP-47 locales commonly supported by browsers
    const langMap: Record<string, string> = {
      hi: 'hi-IN', // Hindi (India) - Default
      en: 'en-IN', // English (India)
      bn: 'bn-IN', // Bengali (India)
      ta: 'ta-IN', // Tamil (India)
      te: 'te-IN', // Telugu (India)
    }
    return langMap[selectedSpeechLang] || 'hi-IN' // Default to Hindi if not found
  }

  // Function to clean text for better TTS (remove emojis, normalize)
  const cleanTextForTTS = (text: string): string => {
    if (!text) return '';
    
    // Remove HTML tags first
    let cleaned = text.replace(/<[^>]*>/g, '');
    
    // Remove markdown formatting but keep the content
    cleaned = cleaned
      .replace(/#{1,6}\s+/g, '') // Remove heading markers
      .replace(/\*\*(.*?)\*\*/g, '$1') // Remove bold markdown
      .replace(/\*(.*?)\*/g, '$1') // Remove italic markdown
      .replace(/\[([^\]]+)\]\(([^\)]+)\)/g, '$1') // Remove links but keep text
      .replace(/`([^`]+)`/g, '$1') // Remove inline code markers
      .replace(/!\[([^\]]+)\]\(([^\)]+)\)/g, '') // Remove image markdown
    
    // Remove zero-width characters and normalize whitespace
    cleaned = cleaned
      .replace(/[\u200B-\u200D\uFEFF]/g, '') // Remove zero-width characters
      .replace(/\s+/g, ' ') // Normalize whitespace
      .trim();
    
    return cleaned;
  }

  // Function to speak text using Vapi AI API
  const speakWithVapi = useCallback(async (text: string, lang: string = 'hi-IN') => {
    // Check if component is still mounted
    if (!naradAIOpen) {
      console.log("Component not mounted, skipping Vapi API");
      setIsSpeaking(false);
      setReadingMessageId(null);
      return;
    }
    
    // Check if audio is enabled
    if (!audioEnabled) {
      console.log('Audio is disabled')
      setIsSpeaking(false)
      setReadingMessageId(null)
      return
    }
    
    console.log('Attempting to speak with Vapi AI API')
    console.log('Text to speak:', text)
    console.log('Language:', lang)
    
    try {
      // Clean text for better TTS
      const cleanText = cleanTextForTTS(text);
      console.log('Clean text to send:', cleanText);
      
      if (!cleanText || cleanText.length < 3) {
        console.log('Text too short or empty after cleaning:', cleanText);
        setError('Text is too short to speak');
        setIsSpeaking(false);
        setReadingMessageId(null);
        return;
      }
      
      // Add character length check
      let finalText = cleanText;
      if (cleanText.length > 2500) {
        console.log('Text too long, truncating:', cleanText.length);
        finalText = cleanText.substring(0, 2500);
      }
      
      console.log('Making request to Vapi AI API');
      
      // Use the comprehensive TTS service
      await ttsService.speakText(finalText, lang);
      
    } catch (error: any) {
      console.error('Error with Vapi AI API:', error);
      setIsSpeaking(false);
      setReadingMessageId(null);
      
      // Provide more specific error messages
      if (error.name === 'NotAllowedError') {
        setError('Audio playback blocked. Please interact with the page first.');
      } else if (error.name === 'AbortError') {
        setError('Audio playback was interrupted.');
      } else {
        setError('Speak feature is not available currently. Please try again.');
      }
    }
  }, [audioRef, naradAIOpen, audioEnabled, setIsSpeaking, setReadingMessageId, setError, cleanTextForTTS, ttsService]);

  // Function to speak text using Eleven Labs API
  const speakWithElevenLabs = useCallback(async (text: string, lang: string = 'hi-IN') => {
    // Check if audio element is available
    if (!audioRef.current) {
      console.log('Audio element not available')
      setIsSpeaking(false)
      setReadingMessageId(null)
      return
    }
    
    // Check if component is still mounted
    if (!naradAIOpen) {
      console.log("Component not mounted, skipping Eleven Labs API");
      setIsSpeaking(false);
      setReadingMessageId(null);
      return;
    }
    
    // Check if audio is enabled
    if (!audioEnabled) {
      console.log('Audio is disabled')
      setIsSpeaking(false)
      setReadingMessageId(null)
      return
    }
    
    console.log('Attempting to speak with Eleven Labs API')
    console.log('Text to speak:', text)
    console.log('Language:', lang)
    
    // Add debugging to see what's being sent
    console.log('ElevenLabs API Key:', process.env.NEXT_PUBLIC_ELEVENLABS_API_KEY ? 'Set' : 'Missing');
    
    try {
      // Map language codes to Eleven Labs voice IDs
      const voiceMap: Record<string, string> = {
        'hi-IN': 'yD0Zg2jxgfQLY8I2MEHO', // Indian Hindi voice (NEW VOICE ID)
        'en-IN': 'yD0Zg2jxgfQLY8I2MEHO', // Using same voice for Indian English
        'bn-IN': 'yD0Zg2jxgfQLY8I2MEHO', // Using same voice for Bengali
        'ta-IN': 'yD0Zg2jxgfQLY8I2MEHO', // Using same voice for Tamil
        'te-IN': 'yD0Zg2jxgfQLY8I2MEHO'  // Using same voice for Telugu
      };
      
      // Get appropriate voice ID
      const voiceId = voiceMap[lang] || 'yD0Zg2jxgfQLY8I2MEHO'; // Default to new voice ID
      console.log('Voice ID:', voiceId);
      
      // Eleven Labs API endpoint
      const url = `https://api.elevenlabs.io/v1/text-to-speech/${voiceId}`;
      
      // Clean text for better TTS
      const cleanText = cleanTextForTTS(text);
      console.log('Clean text to send:', cleanText);
      
      if (!cleanText || cleanText.length < 3) {
        console.log('Text too short or empty after cleaning:', cleanText);
        setError('Text is too short to speak');
        setIsSpeaking(false);
        setReadingMessageId(null);
        return;
      }
      
      // Add character length check (ElevenLabs has limits)
      let finalText = cleanText;
      if (cleanText.length > 2500) {
        console.log('Text too long, truncating:', cleanText.length);
        finalText = cleanText.substring(0, 2500);
      }
      
      console.log('Making request to Eleven Labs API with voice:', voiceId);
      
      // Make request to Eleven Labs API
      const response = await fetch(url, {
        method: 'POST',
        headers: {
          'Accept': 'audio/mpeg',
          'Content-Type': 'application/json',
          'xi-api-key': process.env.NEXT_PUBLIC_ELEVENLABS_API_KEY || '' // API key from environment
        },
        body: JSON.stringify({
          text: finalText,
          model_id: 'eleven_monolingual_v1',
          voice_settings: {
            stability: 0.5,
            similarity_boost: 0.5
          }
        })
      });
      
      if (!response.ok) {
        const errorText = await response.text();
        console.error('ElevenLabs API Error:', response.status, errorText);
        
        // Handle specific error cases
        if (response.status === 401) {
          setError('Invalid ElevenLabs API key. Please check your configuration.');
        } else if (response.status === 400) {
          setError('Bad request to ElevenLabs API. Text may be invalid.');
        } else if (response.status === 429) {
          setError('Rate limit exceeded for ElevenLabs API. Please try again later.');
        } else {
          setError(`ElevenLabs API error: ${response.status}. Please try again.`);
        }
        
        setIsSpeaking(false);
        setReadingMessageId(null);
        return;
      }
      
      // Get audio blob
      const audioBlob = await response.blob();
      const audioUrl = URL.createObjectURL(audioBlob);
      
      // Set up audio element
      audioRef.current.src = audioUrl;
      
      // Add event listeners with proper error handling
      const handlePlay = () => {
        console.log('Eleven Labs audio playback started');
        // Don't reset state here, audio is actually playing
      };
      
      const handleEnded = () => {
        console.log('Eleven Labs audio playback ended');
        setIsSpeaking(false);
        setReadingMessageId(null);
        URL.revokeObjectURL(audioUrl); // Clean up object URL
    
        // Remove event listeners to prevent memory leaks
        if (audioRef.current) {
          audioRef.current.removeEventListener('play', handlePlay);
          audioRef.current.removeEventListener('ended', handleEnded);
          audioRef.current.removeEventListener('error', handleError);
        }
    
        // Process any remaining items in the queue
        setTimeout(() => {
          processSpeechQueue(lang);
        }, 100);
      };
      
      const handleError = (error: any) => {
        console.error('Eleven Labs audio playback error:', error);
        setIsSpeaking(false);
        setReadingMessageId(null);
        setError('Audio playback failed. Please try again.');
        URL.revokeObjectURL(audioUrl); // Clean up object URL
    
        // Remove event listeners to prevent memory leaks
        if (audioRef.current) {
          audioRef.current.removeEventListener('play', handlePlay);
          audioRef.current.removeEventListener('ended', handleEnded);
          audioRef.current.removeEventListener('error', handleError);
        }
    
        // Process any remaining items in the queue
        setTimeout(() => {
          processSpeechQueue(lang);
        }, 100);
      };
      
      // Add event listeners
      audioRef.current.addEventListener('play', handlePlay);
      audioRef.current.addEventListener('ended', handleEnded);
      audioRef.current.addEventListener('error', handleError);
      
      // Play audio
      await audioRef.current.play();
      
    } catch (error: any) {
      console.error('Error with Eleven Labs API:', error);
      setIsSpeaking(false);
      setReadingMessageId(null);
      
      // Provide more specific error messages
      if (error.name === 'NotAllowedError') {
        setError('Audio playback blocked. Please interact with the page first.');
      } else if (error.name === 'AbortError') {
        setError('Audio playback was interrupted.');
      } else {
        setError('Speak feature is not available currently. Please try again.');
      }
    }
  }, [audioRef, naradAIOpen, audioEnabled, setIsSpeaking, setReadingMessageId, setError, cleanTextForTTS, getSpeechLang]);

  // Function to add speech to queue to prevent interruptions
  const processSpeechQueue = useCallback(async (lang?: string) => {
    console.log('processSpeechQueue called with lang:', lang);
    console.log('Current queue length:', speechQueueRef.current.length);
    console.log('Is speaking queue ref:', isSpeakingQueueRef.current);
    
    // If already processing or queue is empty, return
    if (isSpeakingQueueRef.current || speechQueueRef.current.length === 0) {
      console.log('Returning early - already processing or queue empty');
      return;
    }
    
    isSpeakingQueueRef.current = true;
    console.log('Starting to process speech queue');
    
    while (speechQueueRef.current.length > 0) {
      const text = speechQueueRef.current.shift();
      console.log('Processing text from queue:', text);
      
      if (text) {
        try {
          // Stop any ongoing speech before starting new speech
          console.log('Stopping any ongoing speech');
          stopAllSpeech();
          
          // Add a small delay to ensure all speech is stopped
          await new Promise(resolve => setTimeout(resolve, 100));
          
          // Get the appropriate language code
          const speechLang = lang || getSpeechLang();
          console.log('Processing speech from queue:', text);
          
          // Validate text before processing
          if (!text.trim()) {
            console.log('Skipping empty text in speech queue');
            continue;
          }
          
          // Clean text before speaking
          const cleanText = cleanTextForTTS(text);
          if (!cleanText) {
            console.log('Skipping invalid text after cleaning:', text);
            continue;
          }
          
          // Set speaking state before starting speech
          console.log('Setting isSpeaking to true');
          setIsSpeaking(true);
          
          // Use the selected TTS service
          console.log('Using', selectedTTS, 'for speech synthesis from queue')
          switch (selectedTTS) {
            case 'vapi':
            case 'vapi-web':
              await speakWithVapi(cleanText, speechLang);
              break;
            case 'elevenlabs':
              await speakWithElevenLabs(cleanText, speechLang);
              break;
            default:
              // Fallback to ElevenLabs if selected TTS is not recognized
              console.log('Unknown TTS provider, falling back to ElevenLabs');
              await speakWithElevenLabs(cleanText, speechLang);
          }
          
          // Wait for speech to complete or timeout
          await new Promise(resolve => {
            let timeoutId: NodeJS.Timeout;
            const checkSpeaking = () => {
              console.log('Checking if still speaking...');
              // Clear timeout if speech has ended
              if (!isSpeaking) {
                console.log('Speech has ended, clearing timeout');
                clearTimeout(timeoutId);
                resolve(null);
              }
              // Otherwise check again in 100ms
              else {
                console.log('Still speaking, checking again in 100ms');
                setTimeout(checkSpeaking, 100);
              }
            };
            
            // Set a timeout to prevent infinite waiting
            timeoutId = setTimeout(() => {
              console.log('Speech timeout reached, forcing completion');
              setIsSpeaking(false);
              setReadingMessageId(null);
              resolve(null);
            }, 30000); // 30 second timeout
            
            // Start checking
            checkSpeaking();
          });
        } catch (error: any) {
          console.error("Error processing speech from queue:", error);
          setError("Speak feature is not available currently, we are working on it");
          setIsSpeaking(false);
          setReadingMessageId(null);
          
          // Continue processing the rest of the queue even if one item fails
          continue;
        }
      }
    }
    
    console.log('Finished processing speech queue');
    isSpeakingQueueRef.current = false;
  }, [isSpeaking, setIsSpeaking, setReadingMessageId, setError, getSpeechLang, selectedTTS, speakWithVapi, speakWithElevenLabs]);

  // Function to add speech to queue to prevent interruptions
  const queueSpeech = useCallback((text: string, lang?: string) => {
    console.log('Queueing speech:', text, 'with language:', lang);
    speechQueueRef.current.push(text);
    // Add a small delay to ensure proper queuing
    setTimeout(() => {
      processSpeechQueue(lang);
    }, 150);
  }, [processSpeechQueue]);

  // Function to speak text (main function) - MODIFIED TO ALWAYS SPEAK WHEN CALLED
  const speakText = useCallback(async (text: string, lang: string = 'hi-IN') => {
    console.log('speakText called with:', { text, lang });
    // Always speak when this function is called, regardless of voiceToVoiceMode
    // Add speech to queue to prevent interruptions
    queueSpeech(text, lang);
  }, [queueSpeech]);
  
  // COMMENT OUT voice-to-voice mode toggle effect
  /*
  useEffect(() => {
    if (!voiceToVoiceMode && isRecording) {
      console.log('Voice-to-voice mode disabled, stopping recording')
      // Add a small delay to ensure state is properly updated
      setTimeout(() => {
        toggleVoiceRecording()
      }, 100)
    }
  }, [voiceToVoiceMode, isRecording, toggleVoiceRecording])
  */

  // Update the cleanup function to clear the speech queue
  useEffect(() => {
    // Initialize speech recognition
    const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition
    if (SpeechRecognition) {
      setSpeechSupported(true)
      const recognition = new SpeechRecognition()
      recognition.continuous = false
      recognition.interimResults = true
      recognition.maxAlternatives = 1
      // Allow automatic language detection by not setting a specific language initially
      // recognition.lang will be set dynamically when starting recording

      recognition.onresult = (event: any) => {
        let interim = ''
        let finalText = ''
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript: string = event.results[i][0].transcript
          if (event.results[i].isFinal) {
            finalText += transcript + ' '
          } else {
            interim += transcript + ' '
          }
        }

        // Lightweight Hinglish handling: trim and normalize spacing
        const normalizedInterim = interim.trim()
        const normalizedFinal = finalText.trim()

        console.log('Recognition result - Interim:', normalizedInterim, 'Final:', normalizedFinal)
        // Log the detected language if available
        if (event.results.length > 0 && event.results[0].length > 0) {
          console.log('Detected language:', event.results[0][0].lang);
        }
        
        // Clear any previous errors when we get successful recognition results
        if (normalizedInterim || normalizedFinal) {
          setError(null)
        }
        
        // Reset silence timer when we get any results
        if (normalizedInterim || normalizedFinal) {
          if (silenceTimerRef.current) {
            console.log('Clearing existing silence timer');
            clearTimeout(silenceTimerRef.current);
          }
          
          // Update input message with interim results
          if (normalizedInterim) {
            setInterimTranscript(normalizedInterim)
          }
          
          // Update input message with final results
          if (normalizedFinal) {
            setInputMessage((prev: string) => `${prev ? prev + ' ' : ''}${normalizedFinal}`.trim())
            setInterimTranscript('')
            
            // Detect language from the recognized text and update the speech language selector
            const detectedLanguage = detectLanguageFromText(normalizedFinal);
            console.log('Detected language from text:', detectedLanguage);
            setSelectedSpeechLang(detectedLanguage);
            
            // In voice-to-voice mode, automatically send the message immediately for final results
            if (voiceToVoiceMode && normalizedFinal && !isProcessingRef.current) {
              console.log('Automatically sending message in voice-to-voice mode for final result')
              isProcessingRef.current = true
              // Immediate send for final results
              setTimeout(() => {
                handleSendMessage()
                // Reset processing flag after a delay to prevent multiple sends
                setTimeout(() => {
                  isProcessingRef.current = false
                }, 500)
              }, 100) // Very short delay
            }
          }
        }
      }

      recognition.onerror = (e: any) => {
        console.log('Speech recognition error:', e)
        const errorMsg = e?.error === 'not-allowed' 
          ? 'Microphone permission denied. Please allow microphone access in your browser settings.' 
          : e?.error === 'no-speech' 
          ? 'No speech detected. Please try speaking louder or check your microphone.' 
          : e?.error === 'audio-capture' 
          ? 'Audio capture error. Please check your microphone connection.' 
          : e?.error === 'network' 
          ? 'Network error occurred during speech recognition.' 
          : `Speech recognition error: ${e?.error || 'Unknown error'}.`;
        
        setError(errorMsg)
        setIsRecording(false)
        console.log('Resetting isProcessingRef due to error')
        isProcessingRef.current = false
        
        // Clear silence timer on error
        if (silenceTimerRef.current) {
          clearTimeout(silenceTimerRef.current);
          silenceTimerRef.current = null;
        }
      }

      recognition.onend = () => {
        console.log('Speech recognition ended')
        setIsRecording(false)
        
        // Clear silence timer on end
        if (silenceTimerRef.current) {
          clearTimeout(silenceTimerRef.current);
          silenceTimerRef.current = null;
        }
      }

      recognitionRef.current = recognition
    } else {
      setSpeechSupported(false)
      console.log('Speech recognition not supported in this browser')
    }
    
    // Initialize speech synthesis
    if ('speechSynthesis' in window) {
      console.log('Speech synthesis is supported')
      synthRef.current = window.speechSynthesis
      
      // Load voices when they become available
      const loadVoices = () => {
        const voices = synthRef.current.getVoices();
        console.log('Available voices loaded:', voices.map((v: any) => v.name + ' (' + v.lang + ')'));
      };
      
      // Chrome loads voices asynchronously
      if (synthRef.current.onvoiceschanged !== undefined) {
        console.log('Setting up voice change listener')
        synthRef.current.onvoiceschanged = loadVoices;
      }
      
      // Load immediately if voices are already available
      console.log('Loading voices immediately')
      setTimeout(() => {
        loadVoices();
      }, 500); // Small delay to ensure initialization
      
      // Also load after 2 seconds as a backup
      setTimeout(() => {
        loadVoices();
      }, 2000);
    } else {
      console.log('Speech synthesis is not supported')
      setAudioEnabled(false); // Disable audio if speech synthesis is not supported
    }
    
    // Create audio element for ElevenLabs (kept for compatibility but not used)
    audioRef.current = new Audio()
    
    // Cleanup function
    return () => {
      if (synthRef.current) {
        synthRef.current.cancel()
      }
      if (audioRef.current) {
        audioRef.current.pause()
        audioRef.current.src = ''
      }
      // Clear silence timer on cleanup
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current);
      }
      
      // Clear speech queue on cleanup
      clearSpeechQueue();
    }
  }, []);

  // Initialize greeting message when component mounts
  useEffect(() => {
    // Only initialize greeting if there are no messages
    if (messages.length === 0) {
      console.log('Initializing with greeting message');
      // Call the store function directly
      useNaradAIStore.getState().initializeWithGreeting();
    }
  }, []); // Empty dependency array means this runs once on mount

  const handleSendMessage = useCallback(async () => {
    console.log('=== handleSendMessage called ===');
    console.log('Input message:', inputMessage);
    console.log('Is loading:', isLoading);
    console.log('Voice-to-voice mode:', voiceToVoiceMode);
    
    if (!inputMessage.trim() || isLoading) {
      console.log('Not sending message - empty or loading');
      return
    }
    
    const message = inputMessage.trim()
    console.log('Sending message:', message)
    setInputMessage('')
    setInterimTranscript('') // Clear interim transcript
    setShowSuggestions(false)
    // Clear any previous errors when sending a new message
    setError(null)
    
    // Detect language from user input and update session context
    const detectedLanguage = detectLanguageFromText(message);
    console.log('Detected language:', detectedLanguage);
    
    // Update session context with detected language
    if (session) {
      updateSessionContext({
        preferences: {
          ...session.context.preferences,
          language: detectedLanguage
        }
      });
    }
    
    // Update selected speech language to match detected language
    setSelectedSpeechLang(detectedLanguage);
    
    // Start session if not active
    if (!isActive) {
      console.log('Starting new session')
      startSession()
    }
    
    try {
      console.log('Calling sendMessage with message:', message);
      await sendMessage(message)
      console.log('Message sent successfully')
    } catch (error) {
      console.error('Failed to send message:', error)
      setError('Failed to send message. Please try again.')
    }
  }, [inputMessage, isLoading, isActive, startSession, sendMessage, voiceToVoiceMode, session, updateSessionContext])
  
  // Function to manually trigger speech for the last AI message
  const handleSpeakLastMessage = useCallback(() => {
    if (messages.length > 0) {
      const lastMessage = messages[messages.length - 1];
      if (lastMessage.type === 'ai' && lastMessage.content) {
        console.log('Manually requesting to speak last AI message:', lastMessage.content);
        // Set the speaking request flag
        isSpeakingRequestedRef.current = true;
        // Use queueSpeech directly instead of speakText to ensure proper queuing
        queueSpeech(lastMessage.content);
      }
    }
  }, [messages, queueSpeech]);
  
  const handleKeyPress = (e: React.KeyboardEvent) => {
    // Allow sending message with Ctrl+Enter even in voice-to-voice mode
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault()
      // In voice-to-voice mode, only send with Ctrl+Enter to avoid conflicts
      if (voiceToVoiceMode && (e.ctrlKey || e.metaKey)) {
        handleSendMessage()
      } else if (!voiceToVoiceMode) {
        handleSendMessage()
      }
    }
    // Stop recording on Escape key
    else if (e.key === 'Escape' && isRecording) {
      toggleVoiceRecording()
    }
  }
  
  // Function to toggle voice recording
  const toggleVoiceRecording = useCallback(() => {
    if (!speechSupported) {
      setError('Speech recognition not supported in this browser.');
      return;
    }
    
    if (isRecording) {
      console.log('Stopping voice recording');
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
      setIsRecording(false);
    } else {
      console.log('Starting voice recording');
      // Clear any previous errors when starting recording
      setError(null);
      if (recognitionRef.current) {
        try {
          // For speech recognition, we want to detect the language automatically
          // So we don't set a specific language, allowing the browser to detect it
          // However, if a specific language is selected, we can use that
          console.log('Starting recognition with language detection');
          recognitionRef.current.lang = ''; // Empty string allows automatic language detection
          recognitionRef.current.start();
          setIsRecording(true);
        } catch (error) {
          console.error('Error starting speech recognition:', error);
          setError('Failed to start voice recording. Please check microphone permissions.');
          setIsRecording(false);
        }
      } else {
        setError('Speech recognition not properly initialized.');
        setIsRecording(false);
      }
    }
  }, [isRecording, speechSupported]);
  
  const handleSuggestionClick = (suggestion: string) => {
    setInputMessage(suggestion)
    setShowSuggestions(false)
    // Auto-send suggestion after a brief delay
    setTimeout(() => {
      handleSendMessage()
    }, 100)
  }

  // Function to clean text for better TTS (remove emojis, normalize)
  const cleanTextForTTS = (text: string): string => {
    if (!text) return '';
    
    // Remove HTML tags first
    let cleaned = text.replace(/<[^>]*>/g, '');
    
    // Remove markdown formatting but keep the content
    cleaned = cleaned
      .replace(/#{1,6}\s+/g, '') // Remove heading markers
      .replace(/\*\*(.*?)\*\*/g, '$1') // Remove bold markdown
      .replace(/\*(.*?)\*/g, '$1') // Remove italic markdown
      .replace(/\[([^\]]+)\]\([^\)]+\)/g, '$1') // Remove links but keep text
      .replace(/`([^`]+)`/g, '$1') // Remove inline code markers
      .replace(/!\[([^\]]+)\]\([^\)]+\)/g, '') // Remove image markdown
    
    // Remove zero-width characters and normalize whitespace
    cleaned = cleaned
      .replace(/[\u200B-\u200D\uFEFF]/g, '') // Remove zero-width characters
      .replace(/\s+/g, ' ') // Normalize whitespace
      .trim();
    
    return cleaned;
  }

  // Function to stop current speech
  const stopSpeaking = () => {
    console.log('Stopping speech...')
    stopAllSpeech();
    
    // Add a delay to ensure all speech is stopped
    setTimeout(() => {
      stopAllSpeech();
    }, 100);
  }

  // Function to get a male voice for the specified language
  const getMaleVoiceForLanguage = (voices: any[], lang: string) => {
    // Preferred male voices for each language
    const preferredVoices: Record<string, string[]> = {
      'hi-IN': ['Google हिन्दी Male', 'Microsoft Ravi Online (Natural) - Hindi (India)', 'Lekha', 'Rishi', 'Ravi', 'Google UK English Male'],
      'en-IN': ['Google UK English Male', 'Microsoft Mark Online (Natural) - English (India)', 'Daniel', 'Google US English Male'],
      'bn-IN': ['Google বাংলা Male', 'Microsoft Bashkar Online (Natural) - Bengali (India)', 'Google UK English Male'],
      'ta-IN': ['Google தமிழ் Male', 'Microsoft Valluvar Online (Natural) - Tamil (India)', 'Google UK English Male'],
      'te-IN': ['Google తెలుగు Male', 'Microsoft Chitra Online (Natural) - Telugu (India)', 'Google UK English Male']
    };
    
    const preferred = preferredVoices[lang] || [];
    
    // First try to find a preferred voice
    for (const voiceName of preferred) {
      const voice = voices.find((v: any) => 
        v.name.includes(voiceName) && (v.name.toLowerCase().includes('male') || v.gender === 'male')
      );
      if (voice) {
        console.log('Found preferred male voice:', voice.name);
        return voice;
      }
    }
    
    // If no preferred voice found, find any male voice for the language
    const languageVoices = voices.filter((voice: any) => 
      (voice.lang === lang || voice.lang === lang.replace('-', '_')) &&
      (voice.name.toLowerCase().includes('male') || voice.gender === 'male')
    );
    
    console.log('Male voices found for language:', languageVoices.map((v: any) => v.name + ' (' + v.lang + ')'));
    
    // If no male voices found for the specific language, try to find any male voice
    if (languageVoices.length === 0) {
      const maleVoices = voices.filter((voice: any) => 
        voice.name.toLowerCase().includes('male') || voice.gender === 'male'
      );
      
      console.log('All male voices found:', maleVoices.map((v: any) => v.name + ' (' + v.lang + ')'));
      
      // Return first male voice if found
      if (maleVoices.length > 0) {
        console.log('Selected fallback male voice:', maleVoices[0].name);
        return maleVoices[0];
      }
    }
    
    // Return first male voice if found, otherwise first voice for the language
    const selectedVoice = languageVoices.length > 0 ? languageVoices[0] : null;
    if (selectedVoice) {
      console.log('Selected voice:', selectedVoice.name);
    } else {
      console.log('No male voice found for language:', lang);
    }
    return selectedVoice;
  };

  const clearSpeechQueue = useCallback(() => {
    console.log('Clearing speech queue');
    speechQueueRef.current = [];
    isSpeakingQueueRef.current = false;
  }, []);
  
  // Function to stop all speech
  const stopAllSpeech = useCallback(() => {
    console.log('Stopping all speech')
    
    // Stop audio (ElevenLabs compatibility)
    if (audioRef.current) {
      try {
        audioRef.current.pause()
        audioRef.current.currentTime = 0
        audioRef.current.src = ''
        console.log('Audio stopped')
      } catch (error) {
        console.error('Error stopping audio:', error)
      }
    }
    
    // Stop Web Speech API
    if (synthRef.current) {
      try {
        synthRef.current.cancel()
        console.log('Web Speech API cancelled')
      } catch (error) {
        console.error('Error cancelling Web Speech API:', error)
      }
    }
    
    // Clear current utterance reference
    currentUtteranceRef.current = null;
    
    // Reset speaking state
    setIsSpeaking(false)
    setReadingMessageId(null)
    
    // Clear any silence timer
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current)
      silenceTimerRef.current = null
    }
    
    // Reset speaking request flag
    isSpeakingRequestedRef.current = false;
    
    // Clear speech queue
    speechQueueRef.current = [];
    isSpeakingQueueRef.current = false;
  }, [setIsSpeaking, setReadingMessageId])
  

    // Check if audio element is available
    if (!audioRef.current) {
      console.log('Audio element not available')
      setIsSpeaking(false)
      setReadingMessageId(null)
      return
    }
    
    // Check if component is still mounted
    if (!naradAIOpen) {
      console.log("Component not mounted, skipping Eleven Labs API");
      setIsSpeaking(false);
      setReadingMessageId(null);
      return;
    }
    
    // Check if audio is enabled
    if (!audioEnabled) {
      console.log('Audio is disabled')
      setIsSpeaking(false)
      setReadingMessageId(null)
      return
    }
    
    console.log('Attempting to speak with Eleven Labs API')
    console.log('Text to speak:', text)
    console.log('Language:', lang)
    
    // Add debugging to see what's being sent
    console.log('ElevenLabs API Key:', process.env.NEXT_PUBLIC_ELEVENLABS_API_KEY ? 'Set' : 'Missing');
    
    try {
      // Map language codes to Eleven Labs voice IDs
      const voiceMap: Record<string, string> = {
        'hi-IN': 'yD0Zg2jxgfQLY8I2MEHO', // Indian Hindi voice (NEW VOICE ID)
        'en-IN': 'yD0Zg2jxgfQLY8I2MEHO', // Using same voice for Indian English
        'bn-IN': 'yD0Zg2jxgfQLY8I2MEHO', // Using same voice for Bengali
        'ta-IN': 'yD0Zg2jxgfQLY8I2MEHO', // Using same voice for Tamil
        'te-IN': 'yD0Zg2jxgfQLY8I2MEHO'  // Using same voice for Telugu
      };
      
      // Get appropriate voice ID
      const voiceId = voiceMap[lang] || 'yD0Zg2jxgfQLY8I2MEHO'; // Default to new voice ID
      console.log('Voice ID:', voiceId);
      
      // Eleven Labs API endpoint
      const url = `https://api.elevenlabs.io/v1/text-to-speech/${voiceId}`;
      
      // Clean text for better TTS
      const cleanText = cleanTextForTTS(text);
      console.log('Clean text to send:', cleanText);
      
      if (!cleanText || cleanText.length < 3) {
        console.log('Text too short or empty after cleaning:', cleanText);
        setError('Text is too short to speak');
        setIsSpeaking(false);
        setReadingMessageId(null);
        return;
      }
      
      // Add character length check (ElevenLabs has limits)
      let finalText = cleanText;
      if (cleanText.length > 2500) {
        console.log('Text too long, truncating:', cleanText.length);
        finalText = cleanText.substring(0, 2500);
      }
      
      console.log('Making request to Eleven Labs API with voice:', voiceId);
      
      // Make request to Eleven Labs API
      const response = await fetch(url, {
        method: 'POST',
        headers: {
          'Accept': 'audio/mpeg',
          'Content-Type': 'application/json',
          'xi-api-key': process.env.NEXT_PUBLIC_ELEVENLABS_API_KEY || '' // API key from environment
        },
        body: JSON.stringify({
          text: finalText,
          model_id: 'eleven_monolingual_v1',
          voice_settings: {
            stability: 0.5,
            similarity_boost: 0.5
          }
        })
      });
      
      if (!response.ok) {
        const errorText = await response.text();
        console.error('ElevenLabs API Error:', response.status, errorText);
        
        // Handle specific error cases
        if (response.status === 401) {
          setError('Invalid ElevenLabs API key. Please check your configuration.');
        } else if (response.status === 400) {
          setError('Bad request to ElevenLabs API. Text may be invalid.');
        } else if (response.status === 429) {
          setError('Rate limit exceeded for ElevenLabs API. Please try again later.');
        } else {
          setError(`ElevenLabs API error: ${response.status}. Please try again.`);
        }
        
        setIsSpeaking(false);
        setReadingMessageId(null);
        return;
      }
      
      // Get audio blob
      const audioBlob = await response.blob();
      const audioUrl = URL.createObjectURL(audioBlob);
      
      // Set up audio element
      audioRef.current.src = audioUrl;
      
      // Add event listeners with proper error handling
      const handlePlay = () => {
        console.log('Eleven Labs audio playback started');
        // Don't reset state here, audio is actually playing
      };
      
      const handleEnded = () => {
        console.log('Eleven Labs audio playback ended');
        setIsSpeaking(false);
        setReadingMessageId(null);
        URL.revokeObjectURL(audioUrl); // Clean up object URL
    
        // Remove event listeners to prevent memory leaks
        if (audioRef.current) {
          audioRef.current.removeEventListener('play', handlePlay);
          audioRef.current.removeEventListener('ended', handleEnded);
          audioRef.current.removeEventListener('error', handleError);
        }
    
        // Process any remaining items in the queue
        setTimeout(() => {
          processSpeechQueue(lang);
        }, 100);
      };
      
      const handleError = (error: any) => {
        console.error('Eleven Labs audio playback error:', error);
        setIsSpeaking(false);
        setReadingMessageId(null);
        setError('Audio playback failed. Please try again.');
        URL.revokeObjectURL(audioUrl); // Clean up object URL
    
        // Remove event listeners to prevent memory leaks
        if (audioRef.current) {
          audioRef.current.removeEventListener('play', handlePlay);
          audioRef.current.removeEventListener('ended', handleEnded);
          audioRef.current.removeEventListener('error', handleError);
        }
    
        // Process any remaining items in the queue
        setTimeout(() => {
          processSpeechQueue(lang);
        }, 100);
      };
      
      // Add event listeners
      audioRef.current.addEventListener('play', handlePlay);
      audioRef.current.addEventListener('ended', handleEnded);
      audioRef.current.addEventListener('error', handleError);
      
      // Play audio
      await audioRef.current.play();
      
    } catch (error: any) {
      console.error('Error with Eleven Labs API:', error);
      setIsSpeaking(false);
      setReadingMessageId(null);
      
      // Provide more specific error messages
      if (error.name === 'NotAllowedError') {
        setError('Audio playback blocked. Please interact with the page first.');
      } else if (error.name === 'AbortError') {
        setError('Audio playback was interrupted.');
      } else {
        setError('Speak feature is not available currently. Please try again.');
      }
    }
  }, [audioRef, naradAIOpen, audioEnabled, setIsSpeaking, setReadingMessageId, setError, cleanTextForTTS, getSpeechLang]);
  
  // Function to speak text using Web Speech API with enhanced male voice preference
  const speakWithWebSpeech = useCallback(async (text: string, lang: string = 'hi-IN') => {
    // Check if speech synthesis is available
    if (!('speechSynthesis' in window) || !synthRef.current) {
      console.log('Web Speech API not available')
      setIsSpeaking(false)
      setReadingMessageId(null)
      return
    }
    
    // Check if audio is enabled
    if (!audioEnabled) {
      console.log('Audio is disabled')
      setIsSpeaking(false)
      setReadingMessageId(null)
      return
    }
    
    // Check if component is still mounted
    if (!naradAIOpen) {
      console.log("Component not mounted, skipping Web Speech API");
      setIsSpeaking(false);
      setReadingMessageId(null);
      return;
    }
    
    console.log('Attempting to speak with Web Speech API')
    console.log('Text to speak:', text)
    console.log('Language:', lang)
    
    // Ensure voices are loaded
    const ensureVoicesLoaded = (): Promise<void> => {
      return new Promise((resolve) => {
        const voices = synthRef.current.getVoices();
        if (voices.length > 0) {
          console.log('Voices already loaded:', voices.map((v: any) => v.name + ' (' + v.lang + ')'));
          resolve();
          return;
        }
        
        console.log('Waiting for voices to load...');
        const voicesChangedHandler = () => {
          const loadedVoices = synthRef.current.getVoices();
          console.log('Voices loaded:', loadedVoices.map((v: any) => v.name + ' (' + v.lang + ')'));
          synthRef.current?.removeEventListener('voiceschanged', voicesChangedHandler);
          resolve();
        };
        
        synthRef.current?.addEventListener('voiceschanged', voicesChangedHandler);
        
        // Timeout fallback
        setTimeout(() => {
          const timeoutVoices = synthRef.current.getVoices();
          console.log('Voice loading timeout, available voices:', timeoutVoices.map((v: any) => v.name + ' (' + v.lang + ')'));
          synthRef.current?.removeEventListener('voiceschanged', voicesChangedHandler);
          resolve();
        }, 3000); // Increased timeout to 3 seconds
      });
    };
    
    // Ensure voices are loaded
    await ensureVoicesLoaded();
    
    // Cancel any ongoing speech (double check)
    synthRef.current.cancel()
    
    // Small delay to ensure cancellation is complete
    await new Promise(resolve => setTimeout(resolve, 100));
    
    // Create speech utterance
    const enhancedText = text.replace(/\.(\s+)/g, '. $1');
    const utterance = new SpeechSynthesisUtterance(enhancedText)
    currentUtteranceRef.current = utterance // Store reference for stopping
    
    // Settings for natural sound - IMPROVED SETTINGS
    utterance.lang = lang
    utterance.volume = 1.0; // Full volume for clarity
    utterance.rate = 1.0;   // Normal speed (was 0.5 which is too slow)
    utterance.pitch = 1.0;  // Normal pitch (was 0.6 which is too low)

    console.log('Attempting to speak with Web Speech API in language:', lang)
    
    // Handle voice selection - directly select male voices
    const voices = synthRef.current.getVoices()
    console.log('Available voices:', voices.map((v: any) => v.name + ' (' + v.lang + ')'))
    
    // Get appropriate male voice for the language
    let selectedVoice = getMaleVoiceForLanguage(voices, lang);
    
    // If no male voice found, try to find any voice for the language
    if (!selectedVoice) {
      console.log('No male voice found, trying to find any voice for language:', lang);
      const languageVoices = voices.filter((voice: any) => 
        voice.lang === lang || voice.lang === lang.replace('-', '_')
      );
      
      if (languageVoices.length > 0) {
        selectedVoice = languageVoices[0];
        console.log('Selected fallback voice:', selectedVoice.name);
      }
    }
    
    // If still no voice found, use the first available voice
    if (!selectedVoice && voices.length > 0) {
      selectedVoice = voices[0];
      console.log('Selected default voice:', selectedVoice.name);
    }
    
    if (selectedVoice) {
      utterance.voice = selectedVoice;
      console.log('Selected voice:', selectedVoice.name);
    } else {
      console.log('No voice found, using default voice');
    }
    
    utterance.onstart = () => {
      console.log('Web Speech API playback started')
      // Don't reset state here, audio is actually playing
    }

    utterance.onend = () => {
      console.log('Web Speech API playback ended')
      // Check if component is still mounted
      if (naradAIOpen) {
        setIsSpeaking(false)
        setReadingMessageId(null); // Clear reading state
        currentUtteranceRef.current = null
        
        // Process any remaining items in the queue
        setTimeout(() => {
          processSpeechQueue(lang);
        }, 100);
      }
    }
    
    utterance.onerror = (event: any) => {
      console.error('Speech synthesis error:', event)
      console.error('Error details:', {
        error: event.error,
        message: event.message,
        type: event.type,
        utterance: event.utterance
      });
      // Check if component is still mounted
      if (naradAIOpen) {
        // Show user-friendly error message
        setError('Speak feature is not available currently, we are working on it');
        setIsSpeaking(false)
        setReadingMessageId(null); // Clear reading state
        currentUtteranceRef.current = null
        
        // Process any remaining items in the queue
        setTimeout(() => {
          processSpeechQueue(lang);
        }, 100);
      }
    }
    
    console.log('Speaking with voice:', utterance.voice?.name || 'default', 'with language:', utterance.voice?.lang || 'default')
    
    // Speak the text
    try {
      synthRef.current.speak(utterance)
    } catch (error) {
      console.error('Error calling speak method:', error)
      if (naradAIOpen) {
        setError('Speak feature is not available currently, we are working on it')
        setIsSpeaking(false)
        setReadingMessageId(null); // Clear reading state
        currentUtteranceRef.current = null
      }
    }
  }, [synthRef, audioEnabled, naradAIOpen, setIsSpeaking, setReadingMessageId, setError, getMaleVoiceForLanguage]);
  
  // Function to manually trigger speech for a specific message
  const handleSpeakMessage = useCallback((messageId: string, text: string) => {
    console.log('handleSpeakMessage called with:', { messageId, text });
    
    // Stop any current speech and start reading this message
    console.log('Starting speech for message:', messageId);
    console.log('Text to speak:', text);
    
    // Clear any existing speech first
    stopAllSpeech();
    
    // Small delay to ensure all speech is stopped
    setTimeout(() => {
      // Set state before queuing speech
      setReadingMessageId(messageId);
      setIsSpeaking(true); // Set speaking state immediately
      isSpeakingRequestedRef.current = true;
      
      // Queue the speech with the correct language
      queueSpeech(text, getSpeechLang());
    }, 150);
  }, [stopAllSpeech, queueSpeech, setReadingMessageId, setIsSpeaking, getSpeechLang]);

  // Add a test function for ElevenLabs
  const testElevenLabs = useCallback(async () => {
    const testText = "Hello, this is a simple test.";
    try {
      console.log('Testing ElevenLabs API with text:', testText);
      await speakWithElevenLabs(testText, 'hi-IN');
      console.log('Test successful');
    } catch (error) {
      console.error('Test failed:', error);
    }
  }, [speakWithElevenLabs]);

  // Speak AI messages when they arrive - COMPLETELY DISABLED AUTO-SPEAK
  useEffect(() => {
    // This useEffect is intentionally left empty to prevent any automatic speaking
    // All speech playback should be triggered manually by the user
    console.log('AI message received, but auto-speak is disabled');
    console.log('Messages count:', messages.length);
    if (messages.length > 0) {
      const lastMessage = messages[messages.length - 1];
      console.log('Last message type:', lastMessage.type);
      console.log('Last message content preview:', lastMessage.content?.substring(0, 50));
    }
  }, [messages]); // Only dependency is messages to log when new messages arrive

  // Memoize button click handlers
  const handleStopSpeaking = useCallback(() => {
    stopAllSpeech();
  }, [stopAllSpeech]);

  const handleToggleVoiceMode = useCallback(() => {
    // COMMENT OUT voice-to-voice mode toggle
    // toggleVoiceToVoiceMode();
    console.log('Voice-to-voice mode is disabled');
  }, []); // Removed toggleVoiceToVoiceMode dependency

  const handleCloseChat = useCallback(() => {
    console.log('Closing chat box, stopping all speech');
    // Stop all speech immediately
    stopAllSpeech();
    
    // Add a small delay to ensure all speech is stopped before closing
    setTimeout(() => {
      setNaradAIOpen(false);
    }, 150); // Small delay to ensure cleanup
  }, [setNaradAIOpen, stopAllSpeech]);

  // Memoize processed messages to prevent re-processing on every render
  const processedMessages = useMemo(() => {
    console.log('Processing messages:', messages);
    const result = messages.map(message => {
      // Ensure content exists before processing
      const content = message.content || '';
      const processedContent = processMarkdown(content);
      return {
        ...message,
        processedContent
      };
    });
    console.log('Processed messages:', result);
    return result;
  }, [messages]);

  // Memoize language options to prevent re-rendering
  const languageOptions = useMemo(() => [
    { value: 'hi', label: 'हिंदी' },
    { value: 'en', label: 'English' },
    { value: 'bn', label: 'বাংলা' },
    { value: 'ta', label: 'தமிழ்' },
    { value: 'te', label: 'తెలుగు' }
  ], []);

  return (
    <div className={`fixed inset-0 z-50 ${naradAIOpen ? 'block' : 'hidden'}`}>
      <div className="absolute inset-0 bg-black bg-opacity-50" onClick={handleCloseChat} />
      <div className="absolute bottom-4 right-4 w-full max-w-md bg-white rounded-2xl shadow-2xl overflow-hidden transform transition-all duration-300 ease-in-out">
        {/* Chat Header */}
        <div className="bg-gradient-to-r from-primary-600 to-secondary-600 p-4 flex items-center justify-between">
          <div className="flex items-center space-x-3">
            <div className="w-10 h-10 bg-white rounded-full flex items-center justify-center">
              <Bot className="text-primary-600" size={20} />
            </div>
            <div>
              <h3 className="text-white font-semibold">Narad AI</h3>
              <p className="text-primary-100 text-xs">Your Cultural Guide</p>
            </div>
          </div>
          <div className="flex items-center space-x-2">
            {/* Language Selection Dropdown */}
            <select
              value={selectedSpeechLang}
              onChange={(e) => setSelectedSpeechLang(e.target.value as 'hi' | 'en' | 'bn' | 'ta' | 'te')}
              className="narad-ai-dropdown"
              title="Select language for speech"
            >
              {languageOptions.map(option => (
                <option key={option.value} value={option.value}>{option.label}</option>
              ))}
            </select>
            {/* TTS Selection Dropdown */}
            <select
              value={selectedTTS}
              onChange={(e) => setSelectedTTS(e.target.value as 'vapi' | 'vapi-web' | 'elevenlabs' | 'web-speech')}
              className="narad-ai-dropdown"
              title="Select Text-to-Speech service"
            >
              <option value="vapi">Vapi AI</option>
              <option value="vapi-web">Vapi Web TTS</option>
              <option value="elevenlabs">ElevenLabs</option>
              <option value="web-speech">Web Speech API</option>
            </select>
            {/* Stop Speaking Button - Always show but disable when not needed */}
            <button
              onClick={handleStopSpeaking}
              disabled={!isSpeaking}
              className={`p-2 rounded-full ${
                isSpeaking 
                  ? 'bg-gradient-to-r from-red-500 to-orange-500 text-white hover:from-red-600 hover:to-orange-600 shadow-md' 
                  : 'bg-gray-300 text-gray-500 cursor-not-allowed'
              } transition-all duration-200 flex items-center justify-center`}
              aria-label={isSpeaking ? "Stop speaking" : "Speech controls"}
              title={isSpeaking ? "Stop AI from speaking" : "Speech controls"}
            >
              {isSpeaking ? <StopCircle size={16} /> : <Volume2 size={16} />}
            </button>
            {/* COMMENT OUT voice mode toggle button */}
            {/*
            <button
              onClick={handleToggleVoiceMode}
              className={`p-2 rounded-full ${voiceToVoiceMode ? 'bg-white text-primary-600' : 'bg-primary-500 text-white'}`}
              aria-label={voiceToVoiceMode ? "Disable voice mode" : "Enable voice mode"}
              title={voiceToVoiceMode ? "Disable voice-to-voice mode" : "Enable voice-to-voice mode"}
            >
              <Headphones size={16} />
            </button>
            */}
            <button
              onClick={handleCloseChat}
              className="p-2 bg-white bg-opacity-20 rounded-full text-white hover:bg-opacity-30 transition-all duration-200"
              aria-label="Close chat"
              title="Close chat"
            >
              <X size={16} />
            </button>
          </div>
        </div>

        {/* Chat Messages */}
        <div className="h-96 overflow-y-auto p-4 bg-gray-50">
          {processedMessages.map((message, index) => {
            return (
              <div
                key={message._id}
                className={`flex mb-4 ${message.type === 'user' ? 'justify-end' : 'justify-start'}`}
              >
                <div
                  className={`max-w-xs lg:max-w-md px-4 py-2 rounded-2xl ${
                    message.type === 'user'
                      ? 'bg-primary-600 text-white rounded-br-none'
                      : 'bg-white text-gray-800 rounded-bl-none shadow-sm'
                  }`}
                >
                  {message.type === 'ai' ? (
                    <div className="flex items-start">
                      <div className="flex-1">
                        {message.processedContent}
                      </div>
                      {/* Read/Stop button for AI messages */}
                      <button
                        onClick={() => handleSpeakMessage(message._id, message.content)}
                        className={`ml-2 p-1 rounded-full ${
                          readingMessageId === message._id && isSpeaking 
                            ? 'bg-gradient-to-r from-red-500 to-orange-500 text-white' 
                            : 'bg-gradient-to-r from-blue-500 to-purple-500 text-white'
                        } transition-all duration-200 shadow-sm`}
                        aria-label={readingMessageId === message._id && isSpeaking ? "Stop reading" : "Read message"}
                        title={readingMessageId === message._id && isSpeaking ? "Stop reading this message" : "Read this message aloud"}
                      >
                        {readingMessageId === message._id && isSpeaking ? (
                          <StopCircle size={16} className="text-white" />
                        ) : (
                          <Volume2 size={16} className="text-white" />
                        )}
                      </button>
                    </div>
                  ) : (
                    message.processedContent
                  )}
                </div>
              </div>
            );
          })}
          {/* Loading indicator when AI is thinking */}
          {isLoading && (
            <div className="flex justify-start mb-4">
              <div className="max-w-xs lg:max-w-md px-4 py-2 rounded-2xl bg-white text-gray-800 rounded-bl-none shadow-sm">
                <div className="flex items-center">
                  <div className="loading-dots">
                    <span></span>
                    <span></span>
                    <span></span>
                  </div>
                  <span className="ml-2 text-gray-500">Narad is thinking...</span>
                </div>
              </div>
            </div>
          )}
          <div ref={messagesEndRef} />
        </div>

        {/* Chat Input */}
        <div className="border-t border-gray-200 p-4 bg-white">
          {error && (
            <div className="mb-3 p-2 bg-red-50 text-red-700 text-sm rounded-lg flex items-center">
              <AlertCircle size={16} className="mr-2" />
              {error}
            </div>
          )}
          <div className="flex items-center space-x-2">
            {/* Speech-to-text recording button */}
            <button
              onClick={toggleVoiceRecording}
              disabled={!speechSupported || !audioEnabled}
              className={`p-2 rounded-full ${
                isRecording
                  ? 'bg-red-500 text-white animate-pulse'
                  : speechSupported && audioEnabled
                  ? 'bg-gradient-to-r from-blue-500 to-purple-500 text-white hover:from-blue-600 hover:to-purple-600'
                  : 'bg-gray-300 text-gray-500 cursor-not-allowed'
              } transition-all duration-200 shadow-md`}
              aria-label={isRecording ? "Stop recording" : "Start recording"}
              title={isRecording ? "Stop voice recording" : "Start voice recording"}
            >
              {isRecording ? <MicOff size={16} /> : <Mic size={16} />}
            </button>
            <div className="flex-1 relative">
              <input
                ref={inputRef}
                type="text"
                value={inputMessage}
                onChange={(e) => setInputMessage(e.target.value)}
                onKeyPress={handleKeyPress}
                placeholder="Ask me about Indian culture..."
                className="w-full px-4 py-2 border border-gray-300 rounded-full focus:outline-none focus:ring-2 focus:ring-primary-500 focus:border-transparent"
                title="Type your message here"
              />
              {interimTranscript && (
                <div className="absolute inset-0 px-4 py-2 text-gray-500 pointer-events-none">
                  {inputMessage} <span className="text-gray-400">{interimTranscript}</span>
                </div>
              )}
            </div>
            <button
              onClick={handleSendMessage}
              disabled={!inputMessage.trim() || isLoading}
              className={`p-2 rounded-full ${
                inputMessage.trim() && !isLoading
                  ? 'bg-gradient-to-r from-green-500 to-teal-500 text-white hover:from-green-600 hover:to-teal-600 shadow-md'
                  : 'bg-gray-300 text-gray-500 cursor-not-allowed'
              } transition-all duration-200`}
              aria-label="Send message"
              title="Send message"
            >
              <Send size={16} />
            </button>
          </div>
          {/* COMMENT OUT voice-to-voice mode indicator */}
          {/*
          {voiceToVoiceMode && (
            <div className="mt-2 flex items-center justify-between bg-blue-50 p-2 rounded-lg">
              <span className="text-sm text-blue-700 flex items-center">
                <Headphones size={14} className="mr-1" />
                Voice-to-voice mode active
              </span>
              <button
                onClick={toggleVoiceToVoiceMode}
                className="text-xs bg-red-500 text-white px-2 py-1 rounded flex items-center"
                title="Stop voice-to-voice mode"
              >
                <StopCircle size={12} className="mr-1" />
                Stop
              </button>
            </div>
          )}
          */}
        </div>
      </div>
    </div>
  );

export default React.memo(NaradAIChatComponent);
